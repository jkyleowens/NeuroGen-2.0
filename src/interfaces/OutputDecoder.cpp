#include "interfaces/OutputDecoder.h"
#include <iostream>
#include <cmath>
#include <algorithm>
#include <random>
#include <numeric>

OutputDecoder::OutputDecoder(const Config& config, 
                             std::shared_ptr<TokenEmbedding> embeddings)
    : config_(config), embeddings_(embeddings) {
    
    initializeProjection();
    
    std::cout << "ðŸ”Š Initialized OutputDecoder with vocab_size=" 
              << config_.vocab_size << ", output_dim=" 
              << config_.output_dim << std::endl;
}

void OutputDecoder::initializeProjection() {
    // Initialize projection matrix with random weights
    static std::random_device rd;
    static std::mt19937 gen(rd());
    // Use Xavier/Glorot initialization scale for better gradients
    float scale = std::sqrt(2.0f / (config_.output_dim + config_.vocab_size));
    static std::normal_distribution<float> dist(0.0f, scale);
    
    projection_matrix_.resize(config_.vocab_size);
    for (int i = 0; i < config_.vocab_size; ++i) {
        projection_matrix_[i].resize(config_.output_dim);
        for (int j = 0; j < config_.output_dim; ++j) {
            projection_matrix_[i][j] = dist(gen);
        }
    }
    
    // Initialize bias
    projection_bias_.resize(config_.vocab_size, 0.0f);
}

std::vector<float> OutputDecoder::decode(const std::vector<float>& neural_output) {
    // Project neural output to vocabulary logits
    auto logits = projectToLogits(neural_output);
    
    // Apply softmax to get probabilities
    auto probabilities = softmax(logits);
    
    return probabilities;
}

std::vector<float> OutputDecoder::projectToLogits(const std::vector<float>& neural_output) {
    std::vector<float> logits(config_.vocab_size);
    
    // Linear projection: logits = W * neural_output + b
    for (int i = 0; i < config_.vocab_size; ++i) {
        float logit = projection_bias_[i];
        
        for (int j = 0; j < config_.output_dim && j < static_cast<int>(neural_output.size()); ++j) {
            // Use fused multiply-add where possible or simple accumulation
            logit += projection_matrix_[i][j] * neural_output[j];
        }
        
        logits[i] = logit;
    }
    
    return logits;
}

std::vector<float> OutputDecoder::softmax(const std::vector<float>& logits) {
    std::vector<float> probabilities(logits.size());
    
    // Find max for numerical stability
    float max_logit = *std::max_element(logits.begin(), logits.end());
    
    // Compute exp and sum
    float sum_exp = 0.0f;
    for (size_t i = 0; i < logits.size(); ++i) {
        probabilities[i] = std::exp(logits[i] - max_logit);
        // Clamp to prevent NaNs or Infs if any weirdness occurs
        if (std::isnan(probabilities[i]) || std::isinf(probabilities[i])) {
            probabilities[i] = 0.0f;
        }
        sum_exp += probabilities[i];
    }
    
    // Normalize
    // If sum_exp is effectively zero, use uniform distribution
    if (sum_exp < 1e-9f) {
        float uniform = 1.0f / logits.size();
        std::fill(probabilities.begin(), probabilities.end(), uniform);
    } else {
        for (size_t i = 0; i < probabilities.size(); ++i) {
            probabilities[i] /= sum_exp;
        }
    }
    
    return probabilities;
}

int OutputDecoder::sampleToken(const std::vector<float>& probabilities) {
    switch (config_.strategy) {
        case SamplingStrategy::GREEDY:
            return greedySample(probabilities);
        case SamplingStrategy::TEMPERATURE:
            return temperatureSample(probabilities);
        case SamplingStrategy::TOP_K:
            return topKSample(probabilities);
        case SamplingStrategy::TOP_P:
            return topPSample(probabilities);
        default:
            return greedySample(probabilities);
    }
}

int OutputDecoder::decodeAndSample(const std::vector<float>& neural_output) {
    auto probabilities = decode(neural_output);
    return sampleToken(probabilities);
}

std::string OutputDecoder::decodeToString(const std::vector<float>& neural_output) {
    int token_id = decodeAndSample(neural_output);
    return embeddings_->getToken(token_id);
}

std::string OutputDecoder::decodeSequence(const std::vector<std::vector<float>>& neural_outputs) {
    std::string result;
    
    for (const auto& output : neural_outputs) {
        std::string token = decodeToString(output);
        if (!result.empty()) {
            result += " ";
        }
        result += token;
    }
    
    return result;
}

int OutputDecoder::greedySample(const std::vector<float>& probabilities) {
    // Return index of maximum probability
    return std::distance(probabilities.begin(), 
                        std::max_element(probabilities.begin(), probabilities.end()));
}

int OutputDecoder::temperatureSample(const std::vector<float>& probabilities) {
    static std::random_device rd;
    static std::mt19937 gen(rd());
    
    // Create discrete distribution from probabilities
    std::discrete_distribution<int> dist(probabilities.begin(), probabilities.end());
    
    return dist(gen);
}

int OutputDecoder::topKSample(const std::vector<float>& probabilities) {
    // Create pairs of (probability, index)
    std::vector<std::pair<float, int>> prob_index;
    prob_index.reserve(probabilities.size());
    
    for (size_t i = 0; i < probabilities.size(); ++i) {
        prob_index.push_back({probabilities[i], static_cast<int>(i)});
    }
    
    // Sort by probability (descending)
    std::sort(prob_index.begin(), prob_index.end(), 
             [](const auto& a, const auto& b) { return a.first > b.first; });
    
    // Keep only top-k
    int k = std::min(config_.top_k, static_cast<int>(prob_index.size()));
    prob_index.resize(k);
    
    // Renormalize
    float sum = 0.0f;
    for (const auto& p : prob_index) {
        sum += p.first;
    }
    
    std::vector<float> top_k_probs;
    top_k_probs.reserve(k);
    for (const auto& p : prob_index) {
        top_k_probs.push_back(p.first / sum);
    }
    
    // Sample from top-k
    static std::random_device rd;
    static std::mt19937 gen(rd());
    std::discrete_distribution<int> dist(top_k_probs.begin(), top_k_probs.end());
    
    int sampled_idx = dist(gen);
    return prob_index[sampled_idx].second;
}

int OutputDecoder::topPSample(const std::vector<float>& probabilities) {
    // Create pairs of (probability, index)
    std::vector<std::pair<float, int>> prob_index;
    prob_index.reserve(probabilities.size());
    
    for (size_t i = 0; i < probabilities.size(); ++i) {
        prob_index.push_back({probabilities[i], static_cast<int>(i)});
    }
    
    // Sort by probability (descending)
    std::sort(prob_index.begin(), prob_index.end(), 
             [](const auto& a, const auto& b) { return a.first > b.first; });
    
    // Keep tokens until cumulative probability exceeds top_p
    float cumulative = 0.0f;
    size_t cutoff = 0;
    
    for (size_t i = 0; i < prob_index.size(); ++i) {
        cumulative += prob_index[i].first;
        cutoff = i + 1;
        if (cumulative >= config_.top_p) {
            break;
        }
    }
    
    prob_index.resize(cutoff);
    
    // Renormalize
    std::vector<float> nucleus_probs;
    nucleus_probs.reserve(cutoff);
    for (const auto& p : prob_index) {
        nucleus_probs.push_back(p.first / cumulative);
    }
    
    // Sample from nucleus
    static std::random_device rd;
    static std::mt19937 gen(rd());
    std::discrete_distribution<int> dist(nucleus_probs.begin(), nucleus_probs.end());
    
    int sampled_idx = dist(gen);
    return prob_index[sampled_idx].second;
}

std::vector<float> OutputDecoder::applyTemperature(const std::vector<float>& logits, 
                                                   float temperature) {
    std::vector<float> scaled_logits(logits.size());
    
    for (size_t i = 0; i < logits.size(); ++i) {
        scaled_logits[i] = logits[i] / temperature;
    }
    
    return scaled_logits;
}

std::vector<int> OutputDecoder::beamSearch(
    const std::vector<std::vector<float>>& neural_outputs, int beam_width) {
    (void)beam_width; // Unused parameter
    
    // Simplified beam search implementation
    // In practice, this would maintain multiple hypotheses
    
    std::vector<int> best_sequence;
    best_sequence.reserve(neural_outputs.size());
    
    for (const auto& output : neural_outputs) {
        auto probabilities = decode(output);
        int token_id = greedySample(probabilities);  // Simplified
        best_sequence.push_back(token_id);
    }
    
    return best_sequence;
}
